{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import shutil\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM, Input, GlobalMaxPooling1D, Conv1D, MaxPooling1D, Embedding, Flatten, Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.monitor_interval = 0\n",
    "tqdm_notebook().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_comments = pd.concat([train.cleaned_comments,test.cleaned_comments])\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(all_comments)\n",
    "# word_index = tokenizer.word_index\n",
    "# print('Found %s unique tokens.' % len(word_index))\n",
    "# train_sequences = tokenizer.texts_to_sequences(train.cleaned_comments)\n",
    "# test_sequences = tokenizer.texts_to_sequences(test.cleaned_comments)\n",
    "# sequence_pad_len=5000\n",
    "# train_data = pad_sequences(train_sequences, maxlen=sequence_pad_len)\n",
    "# test_data = pad_sequences(test_sequences, maxlen=sequence_pad_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_comment_len = 0\n",
    "# big_lim = 4000\n",
    "# big_lens = {'set_name':list(),'index':list(),'comment_len':list()}\n",
    "# i=0\n",
    "# for s in train_sequences:\n",
    "#     max_comment_len=max(max_comment_len, len(s))\n",
    "#     if len(s) > big_lim:\n",
    "#         big_lens['set_name'].append('train')\n",
    "#         big_lens['index'].append(i)\n",
    "#         big_lens['comment_len'].append(len(s))\n",
    "#     i += 1\n",
    "# i=0\n",
    "# for s in test_sequences:\n",
    "#     max_comment_len=max(max_comment_len, len(s))\n",
    "#     if len(s) > big_lim:\n",
    "#         big_lens['set_name'].append('test')\n",
    "#         big_lens['index'].append(i)\n",
    "#         big_lens['comment_len'].append(len(s))\n",
    "#     i += 1\n",
    "# big_lens_df = pd.DataFrame(big_lens)\n",
    "# max_comment_len\n",
    "# import seaborn as sn\n",
    "# sn.distplot(a=big_lens_df.comment_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_dim = 25\n",
    "# word2vec = KeyedVectors.load_word2vec_format(\n",
    "#             os.path.join(\"W2V\",\n",
    "#                 'w2v.twitter.27B.%dd.txt' % embedding_dim),\n",
    "#             binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "# for word, i in word_index.items():\n",
    "#     if word in word2vec.wv.vocab:\n",
    "#         embedding_matrix[i] = word2vec.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.backend.clear_session()\n",
    "# conv_model = Sequential()\n",
    "# conv_model.add(Embedding(len(word_index) + 1, \n",
    "#                             embedding_dim, \n",
    "#                             weights=[embedding_matrix], \n",
    "#                             input_length=sequence_pad_len)\n",
    "#               )\n",
    "# conv_model.add(Conv1D(128, 5, activation='relu'))\n",
    "# conv_model.add(MaxPooling1D(5))\n",
    "# conv_model.add(Conv1D(128, 5, activation='relu'))\n",
    "# conv_model.add(MaxPooling1D(5))\n",
    "# conv_model.add(Conv1D(128, 5, activation='relu'))\n",
    "# conv_model.add(MaxPooling1D(35))\n",
    "# conv_model.add(Flatten())\n",
    "# conv_model.add(Dense(128, activation='relu'))\n",
    "# conv_model.add(Dense(128, activation='relu'))\n",
    "# conv_model.add(Dense(len(cats), activation='hard_sigmoid'))\n",
    "\n",
    "# model.compile(loss='mean_squared_error',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['acc', custom_metric])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x=train_data, y=Y_train, batch_size=32, sample_weight=samp_weight,epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_model(model, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = compute_sample_weight('balanced', Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "#input layer\n",
    "MLP_model = Sequential()\n",
    "MLP_model.add(Dense(256, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "#hidden layers\n",
    "MLP_model.add(Dense(256, activation='relu'))\n",
    "MLP_model.add(Dense(256, activation='relu'))\n",
    "MLP_model.add(Dense(256, activation='relu'))\n",
    "#output layer\n",
    "# MLP_model.add(Dense(len(cats), activation='hard_sigmoid'))\n",
    "MLP_model.add(Dense(1, activation='hard_sigmoid'))\n",
    "\n",
    "MLP_model.compile(optimizer='adam',\n",
    "             loss='mse',\n",
    "             metrics=['acc', custom_metric])\n",
    "MLP_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model.fit(x=X_train, y=Y_train[cat], batch_size=256, sample_weight=sample_weights, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP_model.optimizer.lr = keras.backend.variable(1e-4, name='lr')\n",
    "MLP_model.fit(x=X_train, y=Y_train[cat], batch_size=256, sample_weight=sample_weights, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model.fit(x=X_train, y=Y_train[cat], batch_size=256, sample_weight=sample_weights, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model.fit(x=X_train, y=Y_train[cat], batch_size=256, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(MLP_model, X_train, cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = selector.transform(cvect.transform(test.cleaned_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_preds = MLP_model.predict(test_input , batch_size=256,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "f = open('%s_preds.P' % cat,'wb')\n",
    "pickle.dump(test_preds, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_model.save('%s.keras.mdl' % cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.where(train['severe_toxic'] == True).where(train['toxic']  != True).dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
